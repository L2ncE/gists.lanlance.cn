## 分布式

### 1. 什么是注册中心

- 服务提供者（RPC Server）：在启动时，向 Registry 注册自身服务，并向 Registry 定期发送心跳汇报存活状态。
- 服务消费者（RPC Client）：在启动时，向 Registry 订阅服务，把 Registry 返回的服务节点列表缓存在本地内存中，并与 RPC Sever 建立连接。
- 服务注册中心（Registry）：用于保存 RPC Server 的注册信息，当 RPC Server 节点发生变更时，Registry 会同步变更，RPC Client 感知后会刷新本地 内存中缓存的服务节点列表。

### 2. CAP 理论

- 一致性(Consistency)：所有节点在同一时间具有相同的数据。
- 可用性(Availability) ：保证每个请求不管成功或者失败都有响应，即服务一直可用，而且是正常响应时间。
- 分隔容忍(Partition tolerance) ：系统中任意信息的丢失或失败不会影响系统的继续运作。

关于 P 的理解，我觉得是在整个系统中某个部分，挂掉了，或者宕机了，并不影响整个系统的运作或者说使用，而可用性是，某个系统的某个节点挂了，但是并不影响系统的接受或者发出请求。

CAP 不可能都取，只能取其中 2 个

### 3. 分布式系统协议 Raft

[从 Raft 原理到实践](https://mp.weixin.qq.com/s?__biz=Mzg3OTU5NzQ1Mw==&mid=2247485759&idx=1&sn=41957e94a2c69426befafd373fbddcc5&chksm=cf034bddf874c2cb52a7aafea5cd194e70308c7d4ad74183db8a36d3747122be1c7a31b84ee3&token=179167416&lang=zh_CN#rd)

### 4. Consul 的主要特征

- CP 模型，使用 Raft 算法来保证强一致性，不保证可用性；
- 支持服务注册与发现、健康检查、KV Store 功能。
- 支持多数据中心，可以避免单数据中心的单点故障，而其部署则需要考虑网络延迟, 分片等情况等。

### 5. Consul 多数据中心

若两个 DataCenter，他们通过 Internet 互联，同时请注意为了提高通信效率，只有 Server 节点才加入跨数据中心的通信。

在单个数据中心中，Consul 分为 Client 和 Server 两种节点（所有的节点也被称为 Agent），Server 节点保存数据，Client 负责健康检查及转发数据请求到 Server；Server 节点有一个 Leader 和多个 Follower，Leader 节点会将数据同步到 Follower，Server 的数量推荐是 3 个或者 5 个，在 Leader 挂掉的时候会启动选举机制产生一个新的 Leader。

集群内的 Consul 节点通过 gossip 协议（流言协议）维护成员关系，也就是说某个节点了解集群内现在还有哪些节点，这些节点是 Client 还是 Server。

集群内数据的读写请求既可以直接发到 Server，也可以通过 Client 使用 RPC 转发到 Server，请求最终会到达 Leader 节点，在允许数据延时的情况下，读请求也可以在普通的 Server 节点完成。

### 6. Consul 的底层通讯协议 Gossip

gossip 协议也称之为流行病协议，它的信息传播行为类似流行病，或者森林的大火蔓延一样，一个接着一个，最终导致全局都收到某一个信息。

在 gossip 协议的网络中，有很多节点交叉分布，当其中的一个节点收到某条信息的时候，它会随机选择周围的几个节点去通知这个信息，收到信息的节点也会接着重复这个过程，直到网络中所有的节点都收到这条信息，才算信息同步完成。

在某个时刻下，网络节点中的信息可能是不对称的，gossip 协议不是一个强一致性的协议，而是最终一致性的协议，理解了这一层，我们去看 consul 的日志的时候，就能有一些端倪了，因为 consul 服务网络在运行的过程中，如果有新的服务注册进来，那么其他的节点会收到某个服务或者节点加入的信息。

### 7. Gossip 的优缺点

优点：

- 扩展性好，加入网络方便
- 容错性好，某个节点离开网络，不会影响整体的消息传播
- 去中心化，Gossip 协议的网络中，不存在中心节点的概念，每个节点都可以成为消息的第一个传播者，只要网络可达，信息就能散播到全网。
- 一致性收敛：这种一传十、十传百的消息传递机制，能够保证消息快速收敛，并保证最终一致性。

缺点：

- 消息延迟：这个是由它的特性决定的，消息的扩散需要时间，这中间各个节点的消息是不一致的。
- 消息冗余：A 节点告知 B 的信息，B 可能会反过来告知 A，这个时候 A 本身已经包含这个消息，却还要处理 B 的请求，这会造成消息的冗余，提高节点处理信息的压力。

### 8. Base 理论

Base 理论的核心思想是最终一致性。

- 基本可用

不追求 CAP 中的「任何时候，读写都是成功的」，而是系统能够基本运行，一直提供服务。基本可用强调了分布式系统在出现不可预知故障的时候，允许损失部分可用性，相比正常的系统，可能是响应时间延长，或者是服务被降级。

- 软状态

软状态可以对应 ACID 事务中的原子性，在 ACID 的事务中，实现的是强制一致性，要么全做要么不做，所有用户看到的数据一致。软状态则是允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。

- 最终一致性

数据不可能一直是软状态，必须在一个时间期限之后达到各个节点的一致性，在期限过后，应当保证所有副本保持数据一致性，也就是达到数据的最终一致性。 在系统设计中，最终一致性实现的时间取决于网络延时、系统负载、不同的存储选型、不同数据复制方案设计等因素。

### 9. Paxos 算法

#### Quorum 选举算法

用一句话解释那就是，在 N 个副本中，一次更新成功的如果有 W 个，那么我在读取数据时是要从大于 N－W 个副本中读取，这样就能至少读到一个更新的数据了。

#### Quorum 的应用

Quorum 机制无法保证强一致性，也就是无法实现任何时刻任何用户或节点都可以读到最近一次成功提交的副本数据。 Quorum 机制的使用需要配合一个获取最新成功提交的版本号的 metadata 服务，这样可以确定最新已经成功提交的版本号，然后从已经读到的数据中就可以确认最新写入的数据。

#### Paxos 的节点角色

- Proposer 提案者
  - 不同的 Proposer 可以提出不同的甚至矛盾的 value，比如某个 Proposer 提议“将变量 X 设置为 1”，另一个 Proposer 提议“将变量 X 设置为 2”，但对同一轮 Paxos 过程，最多只有一个 value 被批准。
- Acceptor 批准者
  - 在集群中，Acceptor 有 N 个，Acceptor 之间完全对等独立，Proposer 提出的 value 必须获得超过半数（N/2+1）的 Acceptor 批准后才能通过。
- Learner 学习者
  - 这里 Leaner 的流程就参考了 Quorum 议会机制，某个 value 需要获得 W=N/2 + 1 的 Acceptor 批准，Learner 需要至少读取 N/2+1 个 Accpetor，最多读取 N 个 Acceptor 的结果后，才能学习到一个通过的 value。
- Client 产生议题者
  - Client 角色，作为产生议题者，实际不参与选举过程，比如发起修改请求的来源等。

#### 选举过程

1. 准备阶段

Proposer 生成全局唯一且递增的 ProposalID，向 Paxos 集群的所有机器发送 Prepare 请求，这里不携带 value，只携带 N 即 ProposalID。 Acceptor 收到 Prepare 请求后，判断收到的 ProposalID 是否比之前已响应的所有提案的 N 大，如果是，则：

- 在本地持久化 N，可记为 Max_N；
- 回复请求，并带上已经 Accept 的提案中 N 最大的 value，如果此时还没有已经 Accept 的提案，则返回 value 为空；
- 做出承诺，不会 Accept 任何小于 Max_N 的提案。 如果否，则不回复或者回复 Error。

2. 选举阶段

Proposer 提出一个提案并发送给 Acceptor；Acceptor 收到提案后会回复 Prepare，如果回复数量大于一半且 value 为空，则 Proposer 发送 Accept 请求，并带上自己指定的 value；如果回复数量大于一半且有的回复 value 不为空，则 Proposer 发送 Accept 请求，并带上 ProposalID 最大的 value；如果回复数量小于等于一半，则 Proposer 尝试更新生成更大的 ProposalID 再进行下一轮；Accpetor 收到 Accept 请求后，如果收到的 N 大于等于 Max_N，则回复提交成功并持久化 N 和 value，否则不回复或者回复提交失败；最后，Proposer 统计所有成功提交的 Accept 回复，如果回复数量大于一半，则表示提交 value 成功，并通知所有 Proposer 和 Learner；否则，尝试更新生成更大的 ProposalID 进入下一轮。

### 9. Paxos 常见的问题

1. 如果半数以内的 Acceptor 失效，如何正常运行？

第一种，如果半数以内的 Acceptor 失效时还没确定最终的 value，此时所有的 Proposer 会重新竞争提案，最终有一个提案会成功提交。

第二种，如果半数以内的 Acceptor 失效时已确定最终的 value，此时所有的 Proposer 提交前必须以最终的 value 提交，也就是 Value 实际已经生效，此值可以被获取，并不再修改。

2. Acceptor 需要接受更大的 N，也就是 ProposalID 有什么意义？

这种机制可以防止其中一个 Proposer 崩溃宕机产生阻塞问题，允许其他 Proposer 用更大 ProposalID 来抢占临时的访问权。

### 10. Zab 与 Paxos 算法的联系与区别

Paxos 的思想在很多分布式组件中都可以看到，Zab 协议可以认为是基于 Paxos 算法实现的，先来看下两者之间的联系：

- 都存在一个 Leader 进程的角色，负责协调多个 Follower 进程的运行
- 都应用 Quorum 机制，Leader 进程都会等待超过半数的 Follower 做出正确的反馈后，才会将一个提案进行提交
- 在 Zab 协议中，Zxid 中通过 epoch 来代表当前 Leader 周期，在 Paxos 算法中，同样存在这样一个标识，叫做 Ballot Number

两者之间的区别是，Paxos 是理论，Zab 是实践，Paxos 是论文性质的，目的是设计一种通用的分布式一致性算法，而 Zab 协议应用在 ZooKeeper 中，是一个特别设计的崩溃可恢复的原子消息广播算法。

Zab 协议增加了崩溃恢复的功能，当 Leader 服务器不可用，或者已经半数以上节点失去联系时，ZooKeeper 会进入恢复模式选举新的 Leader 服务器，使集群达到一个一致的状态。

### 11. 基于消息补偿的最终一致性

（1）系统收到下单请求，将订单业务数据存入到订单库中，并且同时存储该订单对应的消息数据，比如购买商品的 ID 和数量，消息数据与订单库为同一库，更新订单和存储消息为一个本地事务，要么都成功，要么都失败。

（2）库存服务通过消息中间件收到库存更新消息，调用库存服务进行业务操作，同时返回业务处理结果。

（3）消息生产方，也就是订单服务收到处理结果后，将本地消息表的数据删除或者设置为已完成。

（4）设置异步任务，定时去扫描本地消息表，发现有未完成的任务则重试，保证最终一致性。

### 12. 对比两阶段提交，三阶段协议有哪些改进？
